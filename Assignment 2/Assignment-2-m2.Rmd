---
title: "Assignment-2-m2"
author: "Astrid NÃ¸rgaard Fonager"
date: "2023-03-09"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(pacman)

pacman::p_load(tidyverse,
        here,
        posterior,
        cmdstanr,
        brms, tidybayes, future, purrr, furrr, prettydoc,
        tidyr,
        patchwork)
```

# FUNCTIONS

```{r agent functions}

# Random bot
random_agent <- function(bias = 0.7) {
  choice <- rbinom(1,1, bias)
  return(choice)
}

# Win-shift-lose-stay agent
win_shift_agent <- function(prev_choice, feedback, rule_following) {
  if (feedback == 0) { #lose stay
    if(prev_choice == 0) { 
      choice = rbinom(1, 1, (1-rule_following))}
    if(prev_choice == 1) { 
      choice = rbinom(1, 1, rule_following)}
      }
  else if (feedback == 1) { #win shift
    if(prev_choice == 0) { 
      choice = rbinom(1, 1, rule_following)}
    if(prev_choice == 1) { 
      choice = rbinom(1, 1, (1-rule_following))}
  }
  return(choice)  
}
```


```{r simulation function RA}

sim_vs_random <- function(agents, trials, rule_following){
  
  feedback <- array(NA, c(agents, trials))
  performance_df <- data_frame()
  
  for (agent in 1:agents){
    self <- rep(NA, trials)
    bot <- rep(NA, trials)
  
    self[1] <- random_agent(0.5)
    
    for(trial in seq(trials)) {
      bot[trial] <- random_agent()
    }
    
    for (trial in 2:trials){
      if(self[trial-1] == bot[trial-1]) {
        feedback[agent, trial] = 1
      } 
      else {
        feedback[agent, trial] = 0
      }
      self[trial] <- win_shift_agent(prev_choice = self[trial-1], 
                                      feedback = feedback[agent, trial], 
                                      rule_following = rule_following)
    }
    
    df_temp <- tibble(agent = agent, self, bot, trial = seq(trials), 
                      feedback = as.numeric(self==bot)) %>% 
      mutate(cumulative_self = cumsum(feedback)/seq_along(feedback),
             cumulative_bot = cumsum(1-feedback)/seq_along(feedback)) 
    
    performance_df <- rbind(performance_df, df_temp)
    
  }
  
  return(performance_df)
}
```


```{r simulation and fitting function}

sim_d_and_fit <- function(seed, trials, rule_following) {
  for (t in seq(trials)) {
    temp <-  sim_vs_random(agents = 1,
                           trials = trials, 
                           rule_following = rule_following)
  }
  
  data <-  list(
    trials = trials,
    choice = lag(temp$self, 1),
    self = temp$self,
    other = temp$bot
  )
  
  data$choice[1] <- 0

  samples <- model$sample(
    data = data,
    seed = seed,
    chains = 1,
    parallel_chains = 1,
    threads_per_chain = 1,
    iter_warmup = 1000,
    iter_sampling = 2000,
    refresh = 0,
    max_treedepth = 20,
    adapt_delta = 0.99,
  )

  draws_df <- as_draws_df(samples$draws())
  df <- tibble(rule_followingEst = draws_df$rule_following_posterior,
               rule_following_prior = draws_df$rule_following_prior,
               rule_followingTrue = rule_following)
  
  return(df)
}

```

# FITTING THE MODEL

```{r import model}
setwd("/Users/astridnorgaardfonager/Documents/CogSci/8th semester/ACM/Assignments/A2/")

## Specify where the model is
file <- file.path("stan/model2.stan")
model <- cmdstan_model(file, 
                     cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1")) 


```


```{r test fitting}
test <- sim_d_and_fit(seed = 1000, trials = 100, rule_following = 0.8)
```

# PLOTTING

```{r}

ggplot(test) +
  geom_density(aes(rule_followingEst), fill="blue", alpha=0.3) +
  geom_density(aes(rule_following_prior), fill="red", alpha=0.3) +
  xlab("Bias towards rule following") + 
  ylab("Posterior Density") +
  theme_classic() + 
  labs(title = "Bias win")
```

